{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; overflow: hidden;\">\n",
    "    <div style=\"width: 150px; float: left;\"> <img src=\"data/D4Sci_logo_ball.png\" alt=\"Data For Science, Inc\" align=\"left\" border=\"0\"> </div>\n",
    "    <div style=\"float: left; margin-left: 10px;\"> \n",
    "        <h1>Applied Probability Theory From Scratch</h1>\n",
    "        <h1>Bayesian Statistics</h1>\n",
    "        <p>Bruno Gonçalves<br/>\n",
    "        <a href=\"http://www.data4sci.com/\">www.data4sci.com</a><br/>\n",
    "        @bgoncalves, @data4sci</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matploltib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-db2246201d84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatploltib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matploltib'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matploltib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string\n",
    "import gzip\n",
    "import math\n",
    "\n",
    "import watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark -n -v -m -g -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the default style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('./d4sci.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Coin Flips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes theorem provides us with a way to update our beliefs based on new information. Let's consider a practical example: Is a given coin biased or not? And what is the correct probability of heads? \n",
    "\n",
    "At first we know nothing about the coin, so any value of the probability p is equally likely. This is known as an uninformative prior. At each observed flip of the coin, we use this new information to update our distribution over values of p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_coins(flips = 1000000, bins=100, dry_run=False):\n",
    "    # Uninformative prior\n",
    "    # We discretize the probability p in 100 bins\n",
    "    prior = np.ones(bins, dtype='float')/bins\n",
    "    \n",
    "    # The likelihood of heads and tails (the update at each observation) is constant\n",
    "    likelihood_heads = np.arange(bins)/float(bins)\n",
    "    likelihood_tails = 1-likelihood_heads\n",
    "    \n",
    "    if dry_run:\n",
    "        print(\"heads:\", likelihood_heads)\n",
    "        print(\"tails:\", likelihood_tails)\n",
    "        print(\"prior:\", prior)\n",
    "    # Generate the biased coin flips\n",
    "    flips = np.random.choice(a=[True, False], size=flips, p=[0.75, 0.25])\n",
    "    \n",
    "    # Update our beliefs\n",
    "    for i, coin in enumerate(flips):\n",
    "        if coin:  # Heads\n",
    "            posterior = prior * likelihood_heads\n",
    "        else:  # Tails\n",
    "            posterior = prior * likelihood_tails\n",
    "\n",
    "        # Normalize\n",
    "        posterior /= np.sum(posterior)\n",
    "\n",
    "        # The posterior is now the new prior\n",
    "        prior = posterior\n",
    "\n",
    "        if dry_run:\n",
    "            print(i, coin, posterior)\n",
    "\n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_coins(5, bins=10, dry_run=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the convergence to the correct value, we plot the posterior after varying numbers of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(100)/float(100), flip_coins(10))\n",
    "plt.plot(np.arange(100)/float(100), flip_coins(100))\n",
    "plt.plot(np.arange(100)/float(100), flip_coins(1000))\n",
    "plt.plot(np.arange(100)/float(100), flip_coins(10000))\n",
    "plt.plot(np.arange(100)/float(100), flip_coins(100000))\n",
    "plt.legend([10, 100, 1000, 10000, 100000])\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('P(p)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by http://stats.stackexchange.com/questions/72774/numerical-example-to-understand-expectation-maximization. We start by defining a function to measure the binomial log-likelihood. The logarithm is used to avoid numerical issues with small numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## E-M Coin Toss Example  ##\n",
    "def get_binomial_log_likelihood(obs,probs):\n",
    "    \"\"\" Return the (log)likelihood of obs, given the probs\"\"\"\n",
    "    # Binomial Distribution Log PDF\n",
    "    # ln (pdf)      = Binomial Coeff * product of probabilities\n",
    "    # ln[f(x|n, p)] =   comb(N,k)    * num_heads*ln(pH) + (N-num_heads) * ln(1-pH)\n",
    "\n",
    "    N = sum(obs);#number of trials  \n",
    "    k = obs[0] # number of heads\n",
    "    binomial_coeff = math.factorial(N) / (math.factorial(N-k) * math.factorial(k))\n",
    "    prod_probs = obs[0]*math.log(probs[0]) + obs[1]*math.log(1-probs[0])\n",
    "    log_lik = binomial_coeff + prod_probs\n",
    "\n",
    "    return log_lik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the 'experiment' we'll be simulating, as given in the EM tutorial paper by Do and Batzoglou\n",
    "- 1st:  Coin B, {HTTTHHTHTH}, 5H,5T\n",
    "- 2nd:  Coin A, {HHHHTHHHHH}, 9H,1T\n",
    "- 3rd:  Coin A, {HTHHHHHTHH}, 8H,2T\n",
    "- 4th:  Coin B, {HTHTTTHHTT}, 4H,6T\n",
    "- 5th:  Coin A, {THHHTHHHTH}, 7H,3T\n",
    "\n",
    "so, from MLE: $p_A(heads) = 0.80$ and $p_B(heads)=0.45$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_counts = np.array([5,9,8,4,7])\n",
    "tail_counts = 10-head_counts\n",
    "experiments = list(zip(head_counts,tail_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the arrays where we'll keep track of the intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pA_heads = np.zeros(100)\n",
    "pA_heads[0] = 0.60\n",
    "\n",
    "pB_heads = np.zeros(100)\n",
    "pB_heads[0] = 0.50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans we're now ready to run the EM algorithm. The flow is exactly what was described in the slides with one pass over all the *experiments* at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.001  \n",
    "j = 0 # iteration counter\n",
    "improvement = float('inf')\n",
    "\n",
    "while (improvement>delta):\n",
    "    expectation_A = np.zeros((len(experiments),2), dtype=float) \n",
    "    expectation_B = np.zeros((len(experiments),2), dtype=float)\n",
    "\n",
    "    wA = []\n",
    "    wB = []\n",
    "\n",
    "    for i in range(0, len(experiments)):\n",
    "        e = experiments[i] # i'th experiment\n",
    "        \n",
    "        # loglikelihood of e given coin A:\n",
    "        ll_A = get_binomial_log_likelihood(e, np.array([pA_heads[j],1-pA_heads[j]])) \n",
    "        \n",
    "        # loglikelihood of e given coin B\n",
    "        ll_B = get_binomial_log_likelihood(e, np.array([pB_heads[j],1-pB_heads[j]])) \n",
    "\n",
    "        # corresponding weight of A proportional to likelihood of A \n",
    "        weightA = math.exp(ll_A) / ( math.exp(ll_A) + math.exp(ll_B) ) \n",
    "\n",
    "        # corresponding weight of B proportional to likelihood of B\n",
    "        weightB = math.exp(ll_B) / ( math.exp(ll_A) + math.exp(ll_B) ) \n",
    "\n",
    "        expectation_A[i] = np.dot(weightA, e) \n",
    "        expectation_B[i] = np.dot(weightB, e)\n",
    "\n",
    "        wA.append(weightA)\n",
    "        wB.append(weightB)\n",
    "\n",
    "    pA_heads[j+1] = sum(expectation_A)[0] / sum(sum(expectation_A)); \n",
    "    pB_heads[j+1] = sum(expectation_B)[0] / sum(sum(expectation_B)); \n",
    "\n",
    "    improvement = ( max( abs(np.array([pA_heads[j+1],pB_heads[j+1]]) - \n",
    "                    np.array([pA_heads[j],pB_heads[j]]) )) )\n",
    "    j = j+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize the results showing both the convergence and the final values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16, 8));\n",
    "axs[0].plot(range(0,j),pA_heads[0:j], '-')\n",
    "axs[0].plot(range(0,j),pB_heads[0:j], '-')\n",
    "axs[0].set_xlabel('iteration')\n",
    "axs[0].set_ylabel('value')\n",
    "axs[0].legend([r'$\\theta_A$', r'$\\theta_B$'])\n",
    "\n",
    "axs[1].plot(range(len(wA)), wA, '-')\n",
    "axs[1].plot(range(len(wB)), wB, '-')\n",
    "axs[1].legend([r'$p_A$', r'$p_B$'])\n",
    "axs[1].set_xlabel('experiment')\n",
    "axs[1].set_ylabel('probability');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider a more practical and uselful application, that of language detection from a piece of text. For this, we will use a dataset published by Google Books that lists how many times each word was used in each year for a large corpus of books Google scanned.\n",
    "\n",
    "We start by building a vector of character distributions for each language. Due to the total size of the google books dataset, we include only a partial file in the data directory. The interested student is encouraged to download all teh files and use the code below to build hers or his own language detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = sorted(set(string.ascii_letters.lower()))\n",
    "\n",
    "dict_char = dict(zip(characters, range(len(characters))))\n",
    "counts = np.zeros(len(characters), dtype='uint64')\n",
    "\n",
    "line_count = 0\n",
    "\n",
    "filename = \"data/googlebooks-eng-all-1gram-20120701-a.gz\"\n",
    "\n",
    "for line in gzip.open(filename, \"rt\"):\n",
    "    fields = line.lower().strip().split()\n",
    "\n",
    "    line_count += 1\n",
    "\n",
    "    if line_count % 100000 == 0:\n",
    "        print(filename, line_count)\n",
    "        break\n",
    "\n",
    "    count = int(fields[2])\n",
    "    word = fields[0]\n",
    "\n",
    "    if \"_\" in word:\n",
    "        continue\n",
    "\n",
    "    letters = [char for char in word if char in characters]\n",
    "\n",
    "    if len(letters) != len(word):\n",
    "        continue\n",
    "\n",
    "    for letter in letters:\n",
    "        if letter not in dict_char:\n",
    "            continue\n",
    "\n",
    "        counts[dict_char[letter]] += count\n",
    "\n",
    "total = np.sum(counts)\n",
    "list_char = list(dict_char.items())\n",
    "list_char.sort(key=lambda x: x[1])\n",
    "\n",
    "for key, value in enumerate(list_char):\n",
    "    print(value[0], counts[key]/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the most common character is the letter *a*. This is an artifact of the fact that we are using the datafile containing only words that start with the letter *a*. If you were to run it on the entire dataset, the resutls shown in the slides would be found. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, I've also included the complete table for all 5 languages in the repository. This is the datset that we will used to build our language detector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by making a quick visualization of the probabiltity distributions for each language. The first step is to load up the language character frequency from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_letter_lang = pd.read_csv('data/table_langs.dat', sep=' ', header=0, index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency of each letter in the English language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(26), P_letter_lang[\"eng\"])\n",
    "plt.xticks(list(range(26)), P_letter_lang.index);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the comparison across all five languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(26), P_letter_lang[\"eng\"], '-')\n",
    "plt.plot(range(26), P_letter_lang[\"fre\"], '-')\n",
    "plt.plot(range(26), P_letter_lang[\"ger\"], '-')\n",
    "plt.plot(range(26), P_letter_lang[\"ita\"], '-')\n",
    "plt.plot(range(26), P_letter_lang[\"spa\"], '-')\n",
    "plt.xticks(list(range(26)), P_letter_lang.index)\n",
    "plt.legend([\"English\", \"French\", \"German\", \"Italian\", \"Spanish\"])\n",
    "plt.xlabel(\"letter\")\n",
    "plt.ylabel(\"P(letter | language)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there definitely some common trends (the letters *q* and *j* are underrepreented across all languages), there are also some significant peaks that will help us discriminate between one language and the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this table of data it is extremely simple to build a Naive Bayes classifier. To do so, one must simply use Bayes Theorem to determine the probability of the language given the characters observed. Here we calculate the likelihoods we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(P_letter_lang):\n",
    "    langs = list(P_letter_lang.columns)\n",
    "\n",
    "    P_letter = P_letter_lang.mean(axis=1)\n",
    "    P_letter /= P_letter.sum()\n",
    "\n",
    "    P_lang_letter = np.array(P_letter_lang)/(P_letter_lang.shape[1]*P_letter.T[:,None])\n",
    "\n",
    "    L_lang_letter = np.log(P_lang_letter.T)\n",
    "\n",
    "    return langs, P_letter, L_lang_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs, P_letter, L_lang_letter = process_data(P_letter_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have all the tools we need to write down our mini detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lang(langs, P_letter, L_lang_letter, text):\n",
    "    counts = np.zeros(26, dtype='int')\n",
    "    pos = dict(zip(P_letter.index, range(26)))\n",
    "\n",
    "    text_counts =  Counter(text).items()\n",
    "\n",
    "    for letter, count in text_counts:\n",
    "        if letter in pos:\n",
    "            counts[pos[letter]] += count\n",
    "\n",
    "    L_text = np.dot(L_lang_letter, counts)\n",
    "    index = np.argmax(L_text)\n",
    "    lang_text = langs[index]\n",
    "    prob = np.exp(L_text[index])/np.sum(np.exp(L_text))*100\n",
    "\n",
    "    return lang_text, prob, L_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's all there is to it. So now let's test our detector with a few past headlines from Google News:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {}\n",
    "texts[\"eng\"] = \"North Korea’s Test of Nuclear Bomb Amplifies a Global Crisis\".lower()\n",
    "texts[\"ita\"] = \"Nucleare, Onu riunisce consiglio sicurezza. E Seul simula attacco alle basi di Kim\".lower()\n",
    "texts[\"fre\"] = \"Corée du Nord : les Etats-Unis prêts à utiliser leurs capacités nucléaires\".lower()\n",
    "texts[\"spa\"] = \"Estados Unidos amenaza con una “respuesta militar masiva” a Corea del Norte\".lower()\n",
    "texts[\"ger\"] = \"Überraschung\".lower()\n",
    "texts[\"ita2\"] = \"Wales lancia la Wikipedia delle news. Contro il fake in campo anche Google\".lower()\n",
    "\n",
    "for lang in texts:\n",
    "    text = texts[lang]\n",
    "    lang_text, prob, L_text = detect_lang(langs, P_letter, L_lang_letter, text)\n",
    "    print(lang, lang_text, prob, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; overflow: hidden;\">\n",
    "     <img src=\"data/D4Sci_logo_full.png\" alt=\"Data For Science, Inc\" align=\"center\" border=\"0\" width=300px> \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
